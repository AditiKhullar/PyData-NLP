{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Getting-Started\" data-toc-modified-id=\"Getting-Started-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Getting Started</a></span></li><li><span><a href=\"#Our-Data\" data-toc-modified-id=\"Our-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Our Data</a></span></li><li><span><a href=\"#Counting\" data-toc-modified-id=\"Counting-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Counting</a></span><ul class=\"toc-item\"><li><span><a href=\"#Counting-Unigrams\" data-toc-modified-id=\"Counting-Unigrams-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Counting Unigrams</a></span><ul class=\"toc-item\"><li><span><a href=\"#Stopwords:-An-Aside\" data-toc-modified-id=\"Stopwords:-An-Aside-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Stopwords: An Aside</a></span></li></ul></li><li><span><a href=\"#Counting-N-grams\" data-toc-modified-id=\"Counting-N-grams-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Counting N-grams</a></span></li><li><span><a href=\"#N-gram-Probability-Distributions\" data-toc-modified-id=\"N-gram-Probability-Distributions-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>N-gram Probability Distributions</a></span></li></ul></li><li><span><a href=\"#N-gram-Language-Models\" data-toc-modified-id=\"N-gram-Language-Models-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>N-gram Language Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Our-Very-Own-Bigram-Language-Model\" data-toc-modified-id=\"Our-Very-Own-Bigram-Language-Model-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Our Very Own Bigram Language Model</a></span></li><li><span><a href=\"#nltk's-Bigram-Language-Model\" data-toc-modified-id=\"nltk's-Bigram-Language-Model-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>nltk's Bigram Language Model</a></span></li></ul></li><li><span><a href=\"#Generating-Sentences!-Making-the-Language-Model-Speak\" data-toc-modified-id=\"Generating-Sentences!-Making-the-Language-Model-Speak-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Generating Sentences! Making the Language Model Speak</a></span><ul class=\"toc-item\"><li><span><a href=\"#&quot;Manual&quot;-Sentence-Generation\" data-toc-modified-id=\"&quot;Manual&quot;-Sentence-Generation-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>\"Manual\" Sentence Generation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Greedy-Method\" data-toc-modified-id=\"Greedy-Method-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Greedy Method</a></span></li><li><span><a href=\"#Random-Method-(Better)\" data-toc-modified-id=\"Random-Method-(Better)-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Random Method (Better)</a></span></li></ul></li><li><span><a href=\"#nltk-Sentence-Generation\" data-toc-modified-id=\"nltk-Sentence-Generation-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>nltk Sentence Generation</a></span></li></ul></li><li><span><a href=\"#Appendices\" data-toc-modified-id=\"Appendices-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Appendices</a></span><ul class=\"toc-item\"><li><span><a href=\"#Appendix-A:-Working-with-Raw-Data\" data-toc-modified-id=\"Appendix-A:-Working-with-Raw-Data-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Appendix A: Working with Raw Data</a></span></li><li><span><a href=\"#Appendix-B:-Smoothing\" data-toc-modified-id=\"Appendix-B:-Smoothing-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Appendix B: Smoothing</a></span></li><li><span><a href=\"#Appendix-C:-Evaluating-Language-Models\" data-toc-modified-id=\"Appendix-C:-Evaluating-Language-Models-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Appendix C: Evaluating Language Models</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**: \n",
    "In this tutorial, we will be creating an **n-gram language model** from scratch and using the language model to generate some text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, let's import a set of libraries we will find useful later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "\n",
    "# for manipulating data\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "\n",
    "# useful nlp methods\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import ngrams\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "nltk.download('punkt')\n",
    "\n",
    "# plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# printing\n",
    "from tabulate import tabulate\n",
    "\n",
    "# downloading data\n",
    "from urllib import request\n",
    "\n",
    "# a function to flatten a list\n",
    "flatten_list = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "# a function to pretty print a list of tuples\n",
    "def pretty_print_tuples(tuples, headers):\n",
    "    '''Pretty print tuples using tabulate.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tuples: list[tuple[str]]\n",
    "        a list of tuples; each tuple must have the same dimensions\n",
    "    headers: list[str]\n",
    "        a list of headers to use; this list must be the same size as the number of elements in each tuple\n",
    "\n",
    "    '''\n",
    "    table = [list(tup) for tup in tuples]\n",
    "    print(tabulate(table, headers = headers, floatfmt=\".5f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Data\n",
    "Our first corpus will be the [Brown Corpus](https://en.wikipedia.org/wiki/Brown_Corpus), a collection of English-language texts from 500 different sources grouped in 15 different categories.(e.g. fiction, news, ... etc.). We can get this particular dataset [directly from nltk](https://www.nltk.org/book/ch02.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('brown')\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conveniently, nltk has methods to give us all the sentences from the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences\n",
    "tokenized_sentences = brown.sents()\n",
    "print('The Brown corpus has {} sentences.'.format(len(tokenized_sentences)))\n",
    "print()\n",
    "print('--Sample sentences--')\n",
    "for i in range(5):\n",
    "     print('>> sentence {}:'.format(i), ' '.join(tokenized_sentences[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also note that these sentences come in a form where into separate words (tokenized) for us. To be able to train a language model, we need to know what words are in the text, so having the data already in this form is very convenient. Notably, this form of tokenization also makes each punctuation its own token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many things we could do to preprocess the text, such as lowercasing all of the words, stemming, or removing punctuation. However, for this exercise, we'll just keep the text as is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key to building an n-gram language model is knowing how to count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's warm up with trying to answer the following question. \n",
    "\n",
    "**What are the most common words (or \"unigrams\") in our corpus?**\n",
    "\n",
    "To do this, we'll use a python object called a [Counter](https://docs.python.org/3/library/collections.html) to store our data. A Counter is like a dictionary, except it has a default value of 0 when the key does not exist. It also has a convenient `most_common` function to get the most common elements from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_counter = Counter()\n",
    "for sentence in tokenized_sentences:\n",
    "    for word in sentence:\n",
    "        unigram_counter[word] += 1\n",
    "\n",
    "print('Our dataset has {} unique words.'.format(len(unigram_counter)))\n",
    "print()\n",
    "print('--Top 10 Unigrams--')\n",
    "print()\n",
    "pretty_print_tuples(unigram_counter.most_common(n=10), ['Unigram', 'Count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one might expect, we see words like \"the\", \"of\", \"and\"... etc. We can also plot this to see the relative frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather the data\n",
    "most_common_words = unigram_counter.most_common(n=50)\n",
    "indexes = np.arange(len(most_common_words))\n",
    "labels = [l for l,v in most_common_words]\n",
    "values = [v for l,v in most_common_words]\n",
    "\n",
    "# create the plot\n",
    "width = 1\n",
    "plt.rcParams['figure.figsize'] = [20, 5]\n",
    "plt.bar(indexes, values, width)\n",
    "plt.xticks(indexes, labels, rotation=30)\n",
    "plt.title('50 Most Common Words in the Brown Corpus')\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows a phenomenon commonly known as [Zipf's law](https://nlp.stanford.edu/IR-book/html/htmledition/zipfs-law-modeling-the-distribution-of-terms-1.html), which says that the most common words show up exponentially more often than less common words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords: An Aside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll notice that a lot of punctuation show up as the top words, as well as [stopwords](https://en.wikipedia.org/wiki/Stop_words) such as \"the\", \"of\", and \"and\". While we won't want to remove stopwords or punctuation in our n-gram model, doing so here can help us find some more interesting top words and understand our data better. While there is no universally-accepted list of stopwords, we'll use the stopword list from nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print('Sample stopwords:', stopwords.words('english')[0:10])\n",
    "nltk_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_counter = Counter()\n",
    "for sentence in tokenized_sentences:\n",
    "    for word in sentence:\n",
    "        if word.lower() not in nltk_stopwords and word.isalpha():\n",
    "            unigram_counter[word] += 1\n",
    "pretty_print_tuples(unigram_counter.most_common(n=10), ['Unigram', 'Count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take this list and again plot the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather the data\n",
    "most_common_words = unigram_counter.most_common(n=50)\n",
    "indexes = np.arange(len(most_common_words))\n",
    "labels = [l for l,v in most_common_words]\n",
    "values = [v for l,v in most_common_words]\n",
    "\n",
    "# create the plot\n",
    "width = 1\n",
    "plt.rcParams['figure.figsize'] = [20, 5]\n",
    "plt.bar(indexes, values, width)\n",
    "plt.xticks(indexes, labels, rotation=30)\n",
    "plt.title('50 Most Common Words in the Brown Corpus Minus Stopwords and Punctuation')\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a lot of generic nouns, adjectives, and verbs. This tells us that we have a fairly generic dataset and that no particular topic (e.g. \"space\") stands out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move to counting bigrams. To do this, we'll need to make a slight modification to our unigram counting code. As a reminder, when counting bigrams, for a sentence like: \"The red dog is Clifford.\", the bigrams in the sentence will be: `The red`, `red dog`, `dog is`, `is Clifford`, and `Clifford .`. If we add the special sentence begin and sentence end tokens (trust us, this will be useful in the future), we will also get `<s> The` and `. </s>`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_BEGIN = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "bigram_counter = Counter()\n",
    "\n",
    "# track the previous word seen\n",
    "previous_word = SENTENCE_BEGIN\n",
    "for sentence in tokenized_sentences:\n",
    "    for word in sentence:\n",
    "        \n",
    "        # increment bigram count\n",
    "        bigram_counter[(previous_word, word)] += 1\n",
    "        \n",
    "        # update previous word\n",
    "        previous_word = word\n",
    "    \n",
    "    # add a bigram with sentence end\n",
    "    bigram_counter[(previous_word, SENTENCE_END)] += 1\n",
    "    \n",
    "    # reset the previous word\n",
    "    previous_word = SENTENCE_BEGIN\n",
    "    \n",
    "pretty_print_tuples(bigram_counter.most_common(n=10), [\"Bigram\", \"Count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk also conveniently provides a function to help us find the n-grams in a sentence. Look how much shorter the code is now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_counter = Counter()\n",
    "for sentence in tokenized_sentences:\n",
    "    ngram_list = ngrams(sentence, 2, pad_right=True, pad_left=True, \n",
    "                     left_pad_symbol=SENTENCE_BEGIN, right_pad_symbol=SENTENCE_END)\n",
    "    for ngram in ngram_list:\n",
    "        bigram_counter[ngram] += 1\n",
    "pretty_print_tuples(bigram_counter.most_common(n=10), [\"Bigram\", \"Count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the most common trigrams in our dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why don't the top bigrams show up in the top trigrams?**\n",
    "* Hint: which of the top bigrams appear in the top trigrams?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Probability Distributions\n",
    "With these counts, we can start creating probability distributions for n-gram language models. \n",
    "\n",
    "Remember that the core principle behind language models is predicting the probability of a word given its context. In n-gram language models, we assume that the \"context\" is the last $n-1$ words. Therefore, we want to find the distribution: \n",
    "\n",
    "$$P(W_k|w_{k-1}, w_{k-2}, ..., w_{k-n+1})$$\n",
    "\n",
    "Let's start by creating a probability distribution needed for a bigram language model.\n",
    "\n",
    "$$P(W_k|w_{k-1})$$\n",
    "\n",
    "We can compute the probabilities from our data by making the following approximation:\n",
    "\n",
    "$$P(w_k|w_{k-1}) \\approx \\frac{count(w_k, w_{k-1})}{\\sum_w count(w, w_{k-1})} = \\frac{count(w_k, w_{k-1})}{count(w_{k-1})}$$\n",
    "\n",
    "This approximation is also called the *Maximum Likelihood Estimator (MLE)*.\n",
    "\n",
    "To be able to do this well, we'll need to store our bigrams slightly differently. Instead of counting each bigram directly, we will map each \"context\" to the number of times each word follows that context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_context_counter = {}\n",
    "for sentence in tokenized_sentences:\n",
    "    for w1, w2 in ngrams(sentence, 2, pad_right=True, pad_left=True, \n",
    "                         left_pad_symbol=SENTENCE_BEGIN, right_pad_symbol=SENTENCE_END):\n",
    "        \n",
    "        # add a counter for the given context if we haven't seen it before\n",
    "        if w1 not in bigram_context_counter:\n",
    "            bigram_context_counter[w1] = Counter()\n",
    "        \n",
    "        # increment: we've seen an example of w2 after w1\n",
    "        bigram_context_counter[w1][w2] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can easily start answering more interesting questions, such as: \n",
    "\n",
    "**What are the most common words that start a sentence?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the blank\n",
    "top_10_starting_words = bigram_context_counter[__TODO__].most_common(n = 10)\n",
    "pretty_print_tuples(top_10_starting_words, ['Word', 'Count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the equation above, we can use these counts to derive a probability distribution.\n",
    "\n",
    "$$P(w_k|w_{k-1}) \\approx \\frac{count(w_k, w_{k-1})}{count(w_{k-1})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_probabilities(word_counter):\n",
    "    '''Returns a probability distribution across the words.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word_counter: Counter\n",
    "        A counter object containing words and their counts\n",
    "    '''\n",
    "    total_counts = sum(word_counter.values())\n",
    "    return {word: count/total_counts for word, count in word_counter.items()}\n",
    "\n",
    "sentence_begin_probs = get_word_probabilities(bigram_context_counter[SENTENCE_BEGIN])\n",
    "sorted_sentence_begin_probs = sorted(sentence_begin_probs.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# show top distributions\n",
    "pretty_print_tuples(sorted_sentence_begin_probs[0:10], ['Word', 'Probability'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the most common words that come after \"blue\"?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the blank\n",
    "pretty_print_tuples(bigram_context_counter[__TODO__].most_common(n = 10), ['Word', 'Count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many times does the word \"blue\" appear?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the blank\n",
    "sum(bigram_context_counter[__TODO__].values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many times does \"blue sky\" appear?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the blank\n",
    "bigram_context_counter[__TODO__][__TODO__]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the probability \"sky\" comes after the word \"blue\"?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the blank\n",
    "get_word_probabilities(bigram_context_counter[__TODO__])[__TODO__]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Very Own Bigram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data structure we made above, we actually have created a bigram language model! To make this clear, let's rename our variable to indicate it's a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_bigram_manual = bigram_context_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the goal of a language model is to be able to assign probabilities to sequences of words. We can do that with a bigram language model via the following formula.\n",
    "\n",
    "$$P(w_1, w_2, ..., w_n) = P(w_2|w_1)* P(w_3|w_2) * ... * P(w_n|w_{n-1})$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the probability of seeing the sentence \"The blue sky is beautiful.\"?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here, we add the sentence begin and sentence end tokens\n",
    "bigrams = ngrams(['The', 'blue', 'sky', 'is', 'beautiful', '.'], 2, pad_right=True, pad_left=True,\n",
    "                 left_pad_symbol=SENTENCE_BEGIN, right_pad_symbol=SENTENCE_END)\n",
    "\n",
    "p = 1\n",
    "for w1, w2 in bigrams:\n",
    "    # update the probability with the bigram probability\n",
    "    p *= get_word_probabilities(lm_bigram_manual[w1])[w2]\n",
    "print('The probability of \"The blue sky is beautiful.\" is:', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's a small number! To help with this, people often will actually use log-probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The log-probability of \"The blue sky is beautiful.\" is:', math.log(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nltk's Bigram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've walked through a basic example of constructing and using our language model, here we'll show how we can create the same language model using [nltk's implementation of n-gram language models](https://www.nltk.org/api/nltk.lm.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "\n",
    "n = 2 # let's create a bigram language model\n",
    "lm_bigram_nltk = MLE(n) \n",
    "len(lm_bigram_nltk.vocab) # we have no words in our vocabulary yet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add some data. To do this, we'll call [padded_everygram_pipeline](https://www.nltk.org/api/nltk.lm.html#nltk.lm.preprocessing.padded_everygram_pipeline) from nltk to get the data into a form that is friendly to the language model object. `train_text` contains every unigram and bigram from each sentence for counting while `text_vocab` contains the vocabulary of our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, text_vocab = padded_everygram_pipeline(n, tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got our training data, let's train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell takes a few minutes to run\n",
    "lm_bigram_nltk.fit(train_text, vocabulary_text=text_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lm_bigram_nltk.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ``lm_bigram_nltk``, we can again answer: \n",
    "\n",
    "**What are the most common words that come after \"blue\"?**\n",
    "\n",
    "Note that the answer here is the same as that found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# due to interesting ways in how tuples work, note that we have to use ('blue', ) instead of 'blue'\n",
    "pretty_print_tuples(lm_bigram_nltk.counts[('blue',)].most_common(n=10), ['Word', 'Count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute probabilities like before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(\"sky\" | \"blue\")$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_bigram_nltk.score('sky', ['blue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the probability below is the same as what we computed before as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now revisit the question. **What is the probability of seeing the sentence \"The blue sky is beautiful.\"?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = ngrams(['The', 'blue', 'sky', 'is', 'beautiful', '.'], 2, pad_right=True, pad_left=True,\n",
    "                 left_pad_symbol=SENTENCE_BEGIN, right_pad_symbol=SENTENCE_END)\n",
    "p = 1\n",
    "for w1, w2 in bigrams:\n",
    "    # update the probability with the bigram probability\n",
    "    p *= lm_bigram_nltk.score(w2, [w1]) # note that the order here is reversed from our function above\n",
    "print('The probability of \"The blue sky is beautiful.\" is:', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Sentences! Making the Language Model Speak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way we could use this language model is to generate random sentences, like this [Automatic CS Paper Generator](https://pdos.csail.mit.edu/archive/scigen/). We'll start with implementing this using our \"manual\" language model to drive the points home, and then use nltk's implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Manual\" Sentence Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way we could do this is start with the token `<s>`, and then keep picking the word most likely to follow the token until we get a `<\\s>` token. This we will call the Greedy Method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_greedy_sentence():\n",
    "    context = SENTENCE_BEGIN\n",
    "    sentence = []\n",
    "    next_word = lm_bigram_manual[context].most_common(n=1)[0][0]\n",
    "    \n",
    "    # keep getting the next most likely word until we get to the </s> token or \n",
    "    # until our sentence is over 50 words long.\n",
    "    while next_word != SENTENCE_END and len(sentence) < 50:\n",
    "        # append the word to the sentence\n",
    "        sentence.append(next_word)\n",
    "        # update the bigram context\n",
    "        context = next_word\n",
    "        # get most likely next word\n",
    "        next_word = lm_bigram_manual[context].most_common(n=1)[0][0]\n",
    "    return sentence\n",
    "print(\" \".join(generate_greedy_sentence()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you think is wrong with this approach?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Method (Better)\n",
    "\n",
    "To get some more diverse sentences, we can generate random sentences by repeatedly computing $P(W_k|w_{k-1}, w_{k-2})$ and then selecting a word at random from that distribution, and repeating until we get a ``</s>`` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_next_word(lm_manual, context):\n",
    "    '''Based on the given n-gram context, get a random next word.'''\n",
    "    word_counts = lm_manual[context]\n",
    "    probabilities = get_word_probabilities(word_counts)\n",
    "    vocabulary = list(word_counts.keys())\n",
    "    return np.random.choice(vocabulary, 1, p=[probabilities[w] for w in vocabulary])[0]\n",
    "\n",
    "def generate_random_sentence(lm_manual, text_seed=None, random_seed=None):\n",
    "    '''Generate a random next sentence based on the seed word. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lm_manual: dict -> Counter\n",
    "        our manually created language model object\n",
    "    text_seed: [str]\n",
    "        a list of strings to seed the sentence with\n",
    "    random_seed: int\n",
    "        an integer seed for the randomization\n",
    "    '''\n",
    "    np.random.seed(random_seed)\n",
    "    context = SENTENCE_BEGIN\n",
    "    sentence = []\n",
    "    if text_seed is None:\n",
    "        # generate a random seed word\n",
    "        next_word = get_random_next_word(lm_manual, context)\n",
    "    else:\n",
    "        # check if it will be possible to generate a language model\n",
    "        if text_seed not in lm_manual:\n",
    "            raise ValueError('Cannot compute! N-gram model has never seen \"{}\" start a sentence before.'.format(text_seed))\n",
    "        next_word = text_seed\n",
    "    \n",
    "    while next_word != SENTENCE_END:\n",
    "        # append the word to the sentence\n",
    "        sentence.append(next_word)\n",
    "        \n",
    "        # update the history\n",
    "        context = next_word\n",
    "\n",
    "        # get the next word\n",
    "        next_word = get_random_next_word(lm_manual, context)\n",
    "    return sentence\n",
    "\n",
    "print(' '.join(generate_random_sentence(lm_bigram_manual, 'The', 8)))\n",
    "print(' '.join(generate_random_sentence(lm_bigram_manual, None, 8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nltk Sentence Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ntlk's language model object makes sentence generation easy! Let's try it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(lm, text_seed, random_seed=None):\n",
    "    '''Generate a random sentence from the given language model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lm: nltk.LanguageModel\n",
    "        an nltk language model object\n",
    "    text_seed: [str]\n",
    "        a list of strings to seed the sentence with\n",
    "    random_seed: int\n",
    "        an integer seed for the randomization\n",
    "    '''\n",
    "    tokens = lm.generate(50, text_seed=text_seed, random_seed=random_seed)\n",
    "\n",
    "    # just take the first sentence\n",
    "    sentence = [] if text_seed is None else text_seed\n",
    "    for t in tokens:\n",
    "        if t == SENTENCE_END:\n",
    "            break\n",
    "        sentence.append(t)\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "print(generate_sentence(lm_bigram_nltk, ['The'], 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendices\n",
    "\n",
    "These sections contain some more details than we'll be going into for the talk. They are some pretty interesting stuff though!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A: Working with Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some datasets won't be conveniently stored in nltk for us, and we may have to process them ourselves to get it into a usable state. For example, here we show how you could get and process the brown corpus in a more raw format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.sls.hawaii.edu/bley-vroman/brown_nolines.txt'\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "raw[0:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains a set of newline-separated (\"\\r\\n\") sentences. In addition, on manual observation, we notice some lines are not broken into sentences perfectly. We will also split on \". \" as a crude way of catching these cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sentences = raw.split('\\r\\n') # split by newline\n",
    "print('--Example line with multiple sentences--')\n",
    "print(raw_sentences[400]) # example line with a period in it\n",
    "\n",
    "raw_sentences = [re.split(r'(?<=\\.) ', sentence) for sentence in raw_sentences] # split by \". \"\n",
    "raw_sentences = flatten_list(raw_sentences)\n",
    "\n",
    "print()\n",
    "print('--Sample sentences--')\n",
    "for i in range(5):\n",
    "     print('>> sentence {}:'.format(i), raw_sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we note two things:\n",
    "\n",
    "1. Some sentences are blank (e.g. sentences 1 and 3). We'll want to remove those.\n",
    "2. Some words are capitalized while others are not. Some are proper nouns while others are the beginning of sentences. To help standardize this, let's lowercase all of the words (e.g. \"Only\" at the beginning of a sentence and \"only\" in the middle of a sentence are still the same).\n",
    "\n",
    "In order to build a language model. We also need to be able to split a sentence into words. Thankfully, there's a function from nltk called word_tokenize that can do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(s):\n",
    "    # remove trailing whitespace\n",
    "    s = s.strip()\n",
    "    \n",
    "    # lowercase all tokens\n",
    "    s = s.lower()\n",
    "    \n",
    "    # split into tokens\n",
    "    s = word_tokenize(s)\n",
    "    \n",
    "    return s\n",
    "\n",
    "# preprocess each sentence and also remove empty sentences\n",
    "tokenized_sentences = [preprocess_sentence(s) for s in raw_sentences]\n",
    "tokenized_sentences = [s for s in tokenized_sentences if len(s) > 0]\n",
    "\n",
    "# print sample sentences\n",
    "print('We have {} sentences in total.'.format(len(tokenized_sentences)))\n",
    "print()\n",
    "print('--Sample tokenized sentences--')\n",
    "for i in range(5):\n",
    "    print('>> sentence {}:'.format(i), tokenized_sentences[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better, now we can use the output to train language models as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix B: Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a motivating example.\n",
    "\n",
    "**What is the probability of seeing the sentence \"The blue sky is beautifully painted.\"?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = ngrams(['The', 'blue', 'sky', 'is', 'beautifully', 'painted', '.'], 2, pad_right=True, pad_left=True,\n",
    "                 left_pad_symbol=SENTENCE_BEGIN, right_pad_symbol=SENTENCE_END)\n",
    "p = 1\n",
    "for w1, w2 in bigrams:\n",
    "    # update the probability with the bigram probability\n",
    "    p *= lm_bigram_nltk.score(w2, [w1]) # note that the order here is reversed from our function above\n",
    "print('The probability of \"The blue sky is beautifully painted.\" is:', p)\n",
    "print('The probability of seeing \"beautifully\" after \"is\" is:', lm_bigram_nltk.score('beautifully', ['is']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One disadvantage of our current model is that there will be many n-grams we haven't seen before (e.g. \"is beautifully\"). Though \"The blue sky is beautifully painted.\" is a reasonable sentence, the probability assigned to it is 0. \n",
    "\n",
    "To account for this, we can try to \"smooth\" the data. In essence, smoothing is giving a very small, but non-zero probability to n-grams we have not seen before. The assumption here is that though we have not seen a particular n-gram before, it doesn't mean it has no chance of occurring. [These slides](https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf) have a good description of the various smoothing methods, and [this paper](https://www.aclweb.org/anthology/P96-1041.pdf) for an interesting analysis of the empirical performance of various smoothing methods.\n",
    "\n",
    "nltk has a couple methods implemented. We'll start with Laplace Smoothing, which adds one to the count of each n-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import Laplace\n",
    "\n",
    "n = 2\n",
    "lm_bigram_laplace = Laplace(n) # we'll start with LaPlace smoothing\n",
    "train_text, text_vocab = padded_everygram_pipeline(n, tokenized_sentences) # these are generators, so you'll need to make them each time\n",
    "lm_bigram_laplace.fit(train_text, vocabulary_text=text_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lm_bigram_laplace.vocab) == len(lm_bigram_nltk.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the probability $P(\"sky\"|\"blue\")$ under LaPlace smoothing? Is it smaller or greater than the probability without LaPlace smoothing?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lm_bigram_laplace.score('sky', ['blue']))\n",
    "lm_bigram_laplace.score('sky', ['blue']) < lm_bigram_nltk.score('sky', ['blue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the probability of the bigram \"is beautifully\" now?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_bigram_laplace.score('beautifully', ['is'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix C: Evaluating Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way language models can be evaluated and compared is through a metric called [perplexity](https://towardsdatascience.com/perplexity-intuition-and-derivation-105dd481c8f3). Perplexity measures how well a probability model predicts a sample (in this case our entire corpus). The lower the perplexity score (the less \"perplexed\" you are), the better. The formula for perplexity is:\n",
    "\n",
    "$$Perplexity(w_1,w_2,...,w_N) = P(w_1,w_2,...,w_N)^{\\frac{-1}{N}}$$\n",
    "\n",
    "In an n-gram lanugage model, we can simplify this to:\n",
    "\n",
    "$$P(w_1,w_2,...,w_N)^{\\frac{-1}{N}} = \\left(\\prod_{i=1}^{N}P(w_i|w_{i-1},w_{i-2}...,w_{i-n+1})\\right)^{\\frac{-1}{N}}$$\n",
    "\n",
    "To demonstrate how we might use perplexity, we'll create a training and test datasets, then evaluate a bigram and trigram language model on the test dataset using perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell takes a few minutes to run\n",
    "train_sentences = tokenized_sentences[0:-100]\n",
    "test_sentences = tokenized_sentences[-100:]\n",
    "\n",
    "def train_and_calculate_perplexity(LMClass, n):\n",
    "    '''Train and evaluate an n-gram language model.\n",
    "    \n",
    "    Trains model with training data on the provided nltk class (LMClass) \n",
    "    and computes perplexity on the test set.\n",
    "    '''\n",
    "    # train language model\n",
    "    lm = LMClass(n)\n",
    "    train_text, text_vocab = padded_everygram_pipeline(n, train_sentences)\n",
    "    lm.fit(train_text, vocabulary_text=text_vocab)\n",
    "    \n",
    "    # compute perplexity on test sentences\n",
    "    all_ngrams = []\n",
    "    for sentence in test_sentences:\n",
    "        all_ngrams.append(ngrams(sentence, lm.order, pad_right=True, pad_left=True,\n",
    "                           left_pad_symbol=SENTENCE_BEGIN, right_pad_symbol=SENTENCE_END))\n",
    "    all_ngrams = flatten_list(all_ngrams)\n",
    "\n",
    "    print('Perplexity of the {}-gram model on the test set:'.format(n), lm.perplexity(all_ngrams))\n",
    "train_and_calculate_perplexity(MLE, 2)\n",
    "train_and_calculate_perplexity(MLE, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, the perplexities on both datasets are infinity. This is because it's likely never seen some of the n-grams in the test sentences before, thus there is an n-gram such that $P(ngram) = 0$ lurking somewhere in our equation. \n",
    "\n",
    "**Before reading the next line, spend a minute to think, how can we get around this issue?**\n",
    "\n",
    "Smoothing! To get around this, let's try using LaPlace smoothing from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_calculate_perplexity(Laplace, 2)\n",
    "train_and_calculate_perplexity(Laplace, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Does this mean our bigram language model is better than our trigram language model?**\n",
    "\n",
    "Not really! It just means that the bigram language model was better at this specific task. In the end, we should be testing our language model on what we really want to be using it for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use perplexity to evaluate how \"good\" a sentence is. For example, let's say we were given two candidate translations for \"el cielo azul\": \"the blue sky\" or \"the sky blue\". Using perplexity from language models can help us determine which one is more likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(lm, phrase):\n",
    "    '''Computes perplexity of the phrase in a given nltk-language model.'''\n",
    "    phrase_ngrams = ngrams(phrase, lm.order, pad_right=False, pad_left=False)\n",
    "    return lm.perplexity(phrase_ngrams)\n",
    "\n",
    "print('Perplexity for \"the blue sky\":', compute_perplexity(lm_trigram, ['the', 'blue', 'sky']))\n",
    "print('Perplexity for \"the sky blue\":', compute_perplexity(lm_trigram, ['the', 'sky', 'blue']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "180.927px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
